{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nylWRAzj9N3O",
    "outputId": "f7c746eb-dbf2-4439-8379-5245e5e15ea5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFeAqfdBEaX6"
   },
   "outputs": [],
   "source": [
    "csv_file = \"pairs.csv\"\n",
    "csv_data = pd.read_csv(csv_file, low_memory = False)#防止弹出警告\n",
    "pairs = pd.DataFrame(csv_data)\n",
    "pairs.columns =['s','bug1','bug2','label']\n",
    "part_8 = pairs.sample(frac = 0.8)\n",
    "part_2 = pairs.drop(part_8.index)\n",
    "train =[part_8['bug1'].values,part_8['bug2'].values]\n",
    "test =[part_2['bug1'].values,part_2['bug2'].values]\n",
    "train_label = part_8['label'].values\n",
    "test_label = part_2['label'].values\n",
    "train_label = np.where(train_label == -1, 0, train_label)\n",
    "test_label = np.where(test_label == -1, 0, test_label)\n",
    "tset = [pairs['bug1'].values,pairs['bug2'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4MycJfjG4u2",
    "outputId": "46fa9aa3-4c3e-4a90-9be6-c7dbd4f39c75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "<ipython-input-79-f1b98282173a>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens_tensor = torch.tensor(input_ids)\n",
      "<ipython-input-79-f1b98282173a>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segments_tensors = torch.tensor(attention_masks)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sent in train[0]:\n",
    " \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0) #句子token\n",
    "attention_masks = torch.cat(attention_masks, dim=0)#分句标志\n",
    "tokens_tensor = torch.tensor(input_ids)\n",
    "segments_tensors = torch.tensor(attention_masks)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)        \n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "token_embeddings0 = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sent in train[1]:\n",
    " \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0) #句子token\n",
    "attention_masks = torch.cat(attention_masks, dim=0)#分句标志\n",
    "tokens_tensor = torch.tensor(input_ids)\n",
    "segments_tensors = torch.tensor(attention_masks)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)        \n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "token_embeddings1 = torch.stack(hidden_states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "933LPgDNtdek"
   },
   "outputs": [],
   "source": [
    "left = tf.convert_to_tensor(torch.sum(token_embeddings1[-4:], dim=0))\n",
    "right = tf.convert_to_tensor(torch.sum(token_embeddings0[-4:], dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "KJ35d_MLtPS9"
   },
   "outputs": [],
   "source": [
    "Labels = tf.one_hot(train_label, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-pNneOy4lOC0",
    "outputId": "82a15312-7071-46f5-dbb8-335bec889f6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 64, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 64, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 62, 64)       147520      ['input_35[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 62, 64)       147520      ['input_36[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_20 (MaxPooling1D  (None, 31, 64)      0           ['conv1d_32[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_22 (MaxPooling1D  (None, 31, 64)      0           ['conv1d_34[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 29, 64)       12352       ['max_pooling1d_20[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 29, 64)       12352       ['max_pooling1d_22[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_21 (MaxPooling1D  (None, 14, 64)      0           ['conv1d_33[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_23 (MaxPooling1D  (None, 14, 64)      0           ['conv1d_35[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 896)          0           ['max_pooling1d_21[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 896)          0           ['max_pooling1d_23[0][0]']       \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 128)          114816      ['flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 128)          114816      ['flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 128)          0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128)          0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 1)            129         ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 1)            129         ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 549,634\n",
      "Trainable params: 549,634\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 5s 195ms/step - loss: 3.0941 - dense_30_loss: 1.5642 - dense_32_loss: 1.5299 - dense_30_accuracy: 0.7443 - dense_32_accuracy: 0.7273\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 1.1349 - dense_30_loss: 0.5879 - dense_32_loss: 0.5470 - dense_30_accuracy: 0.7045 - dense_32_accuracy: 0.7330\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 0.8908 - dense_30_loss: 0.4816 - dense_32_loss: 0.4092 - dense_30_accuracy: 0.8011 - dense_32_accuracy: 0.8580\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.6703 - dense_30_loss: 0.3450 - dense_32_loss: 0.3253 - dense_30_accuracy: 0.8864 - dense_32_accuracy: 0.8466\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.4915 - dense_30_loss: 0.2582 - dense_32_loss: 0.2332 - dense_30_accuracy: 0.8864 - dense_32_accuracy: 0.9318\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 0.3440 - dense_30_loss: 0.1898 - dense_32_loss: 0.1542 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9773\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.2750 - dense_30_loss: 0.1395 - dense_32_loss: 0.1355 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9659\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.2597 - dense_30_loss: 0.1456 - dense_32_loss: 0.1140 - dense_30_accuracy: 0.9489 - dense_32_accuracy: 0.9659\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.2639 - dense_30_loss: 0.1468 - dense_32_loss: 0.1171 - dense_30_accuracy: 0.9489 - dense_32_accuracy: 0.9602\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.2094 - dense_30_loss: 0.1072 - dense_32_loss: 0.1022 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9659\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.2367 - dense_30_loss: 0.1281 - dense_32_loss: 0.1086 - dense_30_accuracy: 0.9432 - dense_32_accuracy: 0.9489\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.2045 - dense_30_loss: 0.1182 - dense_32_loss: 0.0863 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9602\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.2113 - dense_30_loss: 0.1240 - dense_32_loss: 0.0873 - dense_30_accuracy: 0.9432 - dense_32_accuracy: 0.9659\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 0.2429 - dense_30_loss: 0.1388 - dense_32_loss: 0.1041 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9489\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.2068 - dense_30_loss: 0.1114 - dense_32_loss: 0.0955 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9716\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.1912 - dense_30_loss: 0.0962 - dense_32_loss: 0.0950 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9432\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.2089 - dense_30_loss: 0.0991 - dense_32_loss: 0.1097 - dense_30_accuracy: 0.9489 - dense_32_accuracy: 0.9318\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.1869 - dense_30_loss: 0.0999 - dense_32_loss: 0.0870 - dense_30_accuracy: 0.9489 - dense_32_accuracy: 0.9716\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.1933 - dense_30_loss: 0.0934 - dense_32_loss: 0.0999 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9432\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.2095 - dense_30_loss: 0.0991 - dense_32_loss: 0.1104 - dense_30_accuracy: 0.9375 - dense_32_accuracy: 0.9602\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.2264 - dense_30_loss: 0.1193 - dense_32_loss: 0.1071 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9602\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.2062 - dense_30_loss: 0.0909 - dense_32_loss: 0.1153 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9432\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.2116 - dense_30_loss: 0.1092 - dense_32_loss: 0.1023 - dense_30_accuracy: 0.9432 - dense_32_accuracy: 0.9545\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.1978 - dense_30_loss: 0.1210 - dense_32_loss: 0.0768 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9602\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.2030 - dense_30_loss: 0.1050 - dense_32_loss: 0.0980 - dense_30_accuracy: 0.9432 - dense_32_accuracy: 0.9432\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.1617 - dense_30_loss: 0.0838 - dense_32_loss: 0.0778 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9602\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 0.1696 - dense_30_loss: 0.0846 - dense_32_loss: 0.0850 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9659\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.1778 - dense_30_loss: 0.0901 - dense_32_loss: 0.0877 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9659\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.2104 - dense_30_loss: 0.0941 - dense_32_loss: 0.1163 - dense_30_accuracy: 0.9432 - dense_32_accuracy: 0.9261\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.1757 - dense_30_loss: 0.0864 - dense_32_loss: 0.0893 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9489\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.1685 - dense_30_loss: 0.0785 - dense_32_loss: 0.0900 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9716\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.1864 - dense_30_loss: 0.0964 - dense_32_loss: 0.0900 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9432\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.1738 - dense_30_loss: 0.0843 - dense_32_loss: 0.0895 - dense_30_accuracy: 0.9432 - dense_32_accuracy: 0.9602\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.1787 - dense_30_loss: 0.0980 - dense_32_loss: 0.0807 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9602\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.1930 - dense_30_loss: 0.0929 - dense_32_loss: 0.1002 - dense_30_accuracy: 0.9716 - dense_32_accuracy: 0.9545\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.1703 - dense_30_loss: 0.0887 - dense_32_loss: 0.0816 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9602\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.1650 - dense_30_loss: 0.0846 - dense_32_loss: 0.0804 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9659\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.1473 - dense_30_loss: 0.0810 - dense_32_loss: 0.0663 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9659\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.1522 - dense_30_loss: 0.0784 - dense_32_loss: 0.0738 - dense_30_accuracy: 0.9716 - dense_32_accuracy: 0.9659\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.1474 - dense_30_loss: 0.0732 - dense_32_loss: 0.0741 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9716\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.1775 - dense_30_loss: 0.1039 - dense_32_loss: 0.0736 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9659\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.1543 - dense_30_loss: 0.0847 - dense_32_loss: 0.0696 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9659\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.1542 - dense_30_loss: 0.0827 - dense_32_loss: 0.0715 - dense_30_accuracy: 0.9489 - dense_32_accuracy: 0.9602\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.1760 - dense_30_loss: 0.0980 - dense_32_loss: 0.0780 - dense_30_accuracy: 0.9545 - dense_32_accuracy: 0.9602\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.1610 - dense_30_loss: 0.0778 - dense_32_loss: 0.0832 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9545\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.1746 - dense_30_loss: 0.0903 - dense_32_loss: 0.0843 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9545\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.1686 - dense_30_loss: 0.0953 - dense_32_loss: 0.0733 - dense_30_accuracy: 0.9489 - dense_32_accuracy: 0.9659\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.1498 - dense_30_loss: 0.0742 - dense_32_loss: 0.0756 - dense_30_accuracy: 0.9716 - dense_32_accuracy: 0.9659\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.1669 - dense_30_loss: 0.0881 - dense_32_loss: 0.0788 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9489\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.1483 - dense_30_loss: 0.0825 - dense_32_loss: 0.0658 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9659\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 0.1563 - dense_30_loss: 0.0888 - dense_32_loss: 0.0675 - dense_30_accuracy: 0.9602 - dense_32_accuracy: 0.9602\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 0.1482 - dense_30_loss: 0.0773 - dense_32_loss: 0.0708 - dense_30_accuracy: 0.9659 - dense_32_accuracy: 0.9602\n",
      "Epoch 53/100\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 0.1572 - dense_30_loss: 0.0857 - dense_32_loss: 0.0714 - dense_30_accuracy: 0.9479 - dense_32_accuracy: 0.9583"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-8bf8c8e4c538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# 训练轮数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;31m# 批次大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# 定义输入张量形状\n",
    "input_shape = (64, 768)\n",
    "\n",
    "# 定义卷积层参数\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "activation = 'relu'\n",
    "pool_size = 2\n",
    "\n",
    "# 定义左侧子网络\n",
    "left_input = Input(input_shape)\n",
    "x = Conv1D(filters, kernel_size, activation=activation)(left_input)\n",
    "x = MaxPooling1D(pool_size)(x)\n",
    "x = Conv1D(filters, kernel_size, activation=activation)(x)\n",
    "x = MaxPooling1D(pool_size)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=activation)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "left_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 定义右侧子网络，其结构与左侧子网络相同\n",
    "right_input = Input(input_shape)\n",
    "x = Conv1D(filters, kernel_size, activation=activation)(right_input)\n",
    "x = MaxPooling1D(pool_size)(x)\n",
    "x = Conv1D(filters, kernel_size, activation=activation)(x)\n",
    "x = MaxPooling1D(pool_size)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=activation)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "right_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 定义孪生网络模型\n",
    "model = Model(inputs=[left_input, right_input], outputs=[left_output, right_output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 100 # 训练轮数\n",
    "batch_size = 32 # 批次大小\n",
    "model.fit([left,right], Labels, epochs=epochs, batch_size=batch_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
